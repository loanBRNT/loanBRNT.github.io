[["Map",1,2,9,10,51,52,75,76,95,96],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.17.1","content-config-digest","4777298faf55afed","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://github.com/loanBRNT\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"always\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{\"light\":\"min-light\",\"dark\":\"catppuccin-frappe\"},\"defaultColor\":false,\"wrap\":true,\"transformers\":[{\"name\":\"@shikijs/transformers:notation-highlight\"},{\"name\":\"@shikijs/transformers:notation-highlight-word\"},{\"name\":\"@shikijs/transformers:notation-diff\"}]},\"remarkPlugins\":[null],\"rehypePlugins\":[null,[null,{\"behavior\":\"prepend\",\"properties\":{\"className\":[\"heading-link\"],\"ariaLabel\":\"Link to section\"},\"content\":{\"type\":\"text\",\"value\":\"#\"}}],[null,{\"target\":\"_blank\",\"rel\":[\"noopener\",\"noreferrer\"]}]],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false,\"svgo\":false},\"legacy\":{\"collections\":false}}","projects",["Map",11,12],"sawyer-bartender",{"id":11,"data":13,"body":26,"filePath":27,"digest":28,"rendered":29},{"title":14,"description":15,"link":16,"github":17,"tags":18,"types":23,"order":25},"Building a Robot Bartender","How I turned a Sawyer manipulator into a bartender and built a no-code system for others to control it","https://www.youtube.com/watch?v=LXb3EKWsInQ","https://github.com/loanBRNT/sawyer_vision_bartender",[19,20,21,22],"Python","ROS/ROS2","Detection","Control",[24],"internship",2,"This project was developed during a research internship at Ostfalia University of Applied Sciences (Germany).\nThe objective was to design a complete robotic bartender system using a [Sawyer manipulator](https://www.rethinkrobotics.com/sawyer), integrated with a [Pepper robot](https://www.aldebaran.com/en/pepper) acting as a waiter. My focus was entirely on the Sawyer application.\n\n## System Overview\n\nI treated this as a full-stack robotics problem:\n\n- **Perception:** I trained and integrated YOLOv5 to identify bottles and cups. While being reliable under the shifting lights of a lab environment.\n- **Manipulation:** I designed the grasp sequencing to handle different bottle shapes and ensure a steady pour without \"spilling\".\n- **The Bridge (No-Code):** I built a Node-RED interface that abstracted the ROS complexity. It allowed Masterâ€™s students to \"program\" the robot for their own experiments.\n\nIt was one of my first autonomous project on a real robot. Supervisors let me explore any solution that I want and I really faced the curse of any real robots application: race conditions ðŸ«¡ and timing issues. \n\n## My Contributions\n\nMy main responsibilities included:\n\n- Designing the end-to-end control pipeline for the Sawyer robot.\n- Training and integrating a YOLOv5-based object detection model for bottles and cups.\n- Implementing grasp and manipulation behaviors for picking cups and pouring from bottles.\n- Designing a Node-RED interface to expose high-level robot actions to non-expert users.\n- Coordinating task execution between Sawyer and Pepper through message-based synchronization.\n\n## Technical Challenges & Design Choices\n\n- **Robust perception**: bottle and cup detection needed to be reliable under varying lighting and clutter.\n- **Grasp sequencing**: handling bottles safely while ensuring stable pouring.\n- **Human-in-the-loop coordination**: synchronizing actions with Pepper or Humans required explicit confirmation handling.\n- **Usability**: the Node-RED interface had to abstract ROS complexity while remaining expressive for students.\n\n## Outcome\n\nThe final system was able to autonomously execute drink-serving sequences, from order recognition to cup delivery, while remaining configurable through a no-code interface.\n\nThis project demonstrates my early experience with:\n- full-stack robotic systems\n- perception-driven manipulation\n- ROS-based orchestration\n- designing interfaces for non-roboticists","site/content/projects/sawyer-bartender.md","ce4b4f6539d70d8f",{"html":30,"metadata":31},"\u003Cp>This project was developed during a research internship at Ostfalia University of Applied Sciences (Germany).\nThe objective was to design a complete robotic bartender system using a \u003Ca href=\"https://www.rethinkrobotics.com/sawyer\" rel=\"noopener noreferrer\" target=\"_blank\">Sawyer manipulator\u003C/a>, integrated with a \u003Ca href=\"https://www.aldebaran.com/en/pepper\" rel=\"noopener noreferrer\" target=\"_blank\">Pepper robot\u003C/a> acting as a waiter. My focus was entirely on the Sawyer application.\u003C/p>\n\u003Ch2 id=\"system-overview\">\u003Ca class=\"heading-link\" aria-label=\"Link to section\" href=\"#system-overview\">#\u003C/a>System Overview\u003C/h2>\n\u003Cp>I treated this as a full-stack robotics problem:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Perception:\u003C/strong> I trained and integrated YOLOv5 to identify bottles and cups. While being reliable under the shifting lights of a lab environment.\u003C/li>\n\u003Cli>\u003Cstrong>Manipulation:\u003C/strong> I designed the grasp sequencing to handle different bottle shapes and ensure a steady pour without â€œspillingâ€.\u003C/li>\n\u003Cli>\u003Cstrong>The Bridge (No-Code):\u003C/strong> I built a Node-RED interface that abstracted the ROS complexity. It allowed Masterâ€™s students to â€œprogramâ€ the robot for their own experiments.\u003C/li>\n\u003C/ul>\n\u003Cp>It was one of my first autonomous project on a real robot. Supervisors let me explore any solution that I want and I really faced the curse of any real robots application: race conditions ðŸ«¡ and timing issues.\u003C/p>\n\u003Ch2 id=\"my-contributions\">\u003Ca class=\"heading-link\" aria-label=\"Link to section\" href=\"#my-contributions\">#\u003C/a>My Contributions\u003C/h2>\n\u003Cp>My main responsibilities included:\u003C/p>\n\u003Cul>\n\u003Cli>Designing the end-to-end control pipeline for the Sawyer robot.\u003C/li>\n\u003Cli>Training and integrating a YOLOv5-based object detection model for bottles and cups.\u003C/li>\n\u003Cli>Implementing grasp and manipulation behaviors for picking cups and pouring from bottles.\u003C/li>\n\u003Cli>Designing a Node-RED interface to expose high-level robot actions to non-expert users.\u003C/li>\n\u003Cli>Coordinating task execution between Sawyer and Pepper through message-based synchronization.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"technical-challenges--design-choices\">\u003Ca class=\"heading-link\" aria-label=\"Link to section\" href=\"#technical-challenges--design-choices\">#\u003C/a>Technical Challenges &#x26; Design Choices\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Robust perception\u003C/strong>: bottle and cup detection needed to be reliable under varying lighting and clutter.\u003C/li>\n\u003Cli>\u003Cstrong>Grasp sequencing\u003C/strong>: handling bottles safely while ensuring stable pouring.\u003C/li>\n\u003Cli>\u003Cstrong>Human-in-the-loop coordination\u003C/strong>: synchronizing actions with Pepper or Humans required explicit confirmation handling.\u003C/li>\n\u003Cli>\u003Cstrong>Usability\u003C/strong>: the Node-RED interface had to abstract ROS complexity while remaining expressive for students.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"outcome\">\u003Ca class=\"heading-link\" aria-label=\"Link to section\" href=\"#outcome\">#\u003C/a>Outcome\u003C/h2>\n\u003Cp>The final system was able to autonomously execute drink-serving sequences, from order recognition to cup delivery, while remaining configurable through a no-code interface.\u003C/p>\n\u003Cp>This project demonstrates my early experience with:\u003C/p>\n\u003Cul>\n\u003Cli>full-stack robotic systems\u003C/li>\n\u003Cli>perception-driven manipulation\u003C/li>\n\u003Cli>ROS-based orchestration\u003C/li>\n\u003Cli>designing interfaces for non-roboticists\u003C/li>\n\u003C/ul>",{"headings":32,"localImagePaths":45,"remoteImagePaths":46,"frontmatter":47,"imagePaths":50},[33,36,39,42],{"depth":25,"slug":34,"text":35},"system-overview","#System Overview",{"depth":25,"slug":37,"text":38},"my-contributions","#My Contributions",{"depth":25,"slug":40,"text":41},"technical-challenges--design-choices","#Technical Challenges & Design Choices",{"depth":25,"slug":43,"text":44},"outcome","#Outcome",[],[],{"title":14,"description":15,"link":16,"youtube":16,"github":17,"tags":48,"types":49,"order":25},[19,20,21,22],[24],[],"publicationsPage",["Map",53,54],"index",{"id":53,"data":55,"body":58,"filePath":59,"digest":60,"rendered":61},{"title":56,"description":57},"Publications","Journal papers, conference papers, and preprints.","## Journal & Conference Papers\n\n*not yet*\n\n## Workshops & Preprints\n\n- **Addressing Long-Horizon failure in Language-Grounded Robotic via Structured Interaction**\n\n  *L. Bernat, A. Herbulot, M. Grard, F. Lamiraux*\n\n  Submitted to RSS 2026\n  \n  \u003Ca class=\"pub-button\" href=\"/research/magma-gen\">Related project\u003C/a>","site/content/publications/index.md","1d1c76a0cb5ab578",{"html":62,"metadata":63},"\u003Ch2 id=\"journal--conference-papers\">\u003Ca class=\"heading-link\" aria-label=\"Link to section\" href=\"#journal--conference-papers\">#\u003C/a>Journal &#x26; Conference Papers\u003C/h2>\n\u003Cp>\u003Cem>not yet\u003C/em>\u003C/p>\n\u003Ch2 id=\"workshops--preprints\">\u003Ca class=\"heading-link\" aria-label=\"Link to section\" href=\"#workshops--preprints\">#\u003C/a>Workshops &#x26; Preprints\u003C/h2>\n\u003Cul>\n\u003Cli>\n\u003Cp>\u003Cstrong>Addressing Long-Horizon failure in Language-Grounded Robotic via Structured Interaction\u003C/strong>\u003C/p>\n\u003Cp>\u003Cem>L. Bernat, A. Herbulot, M. Grard, F. Lamiraux\u003C/em>\u003C/p>\n\u003Cp>Submitted to RSS 2026\u003C/p>\n\u003Cp>\u003Ca class=\"pub-button\" href=\"/research/magma-gen\">Related project\u003C/a>\u003C/p>\n\u003C/li>\n\u003C/ul>",{"headings":64,"localImagePaths":71,"remoteImagePaths":72,"frontmatter":73,"imagePaths":74},[65,68],{"depth":25,"slug":66,"text":67},"journal--conference-papers","#Journal & Conference Papers",{"depth":25,"slug":69,"text":70},"workshops--preprints","#Workshops & Preprints",[],[],{"title":56,"description":57},[],"about",["Map",53,77],{"id":53,"data":78,"body":80,"filePath":81,"digest":82,"rendered":83},{"title":79},"Because Intelligence starts with curiosity","I am a **PhD Researcher in Robotics** working on foundation models and long-horizon manipulation. Most of my days are spent thinking about language-conditioned tasks and how multi-agent systems can work together. I believe that true '*intelligence*' won't emerge simply from scaling parameters, but through **System-level Intelligence**. Because of this, I want to build robust, modular systems that don't just work in a paper, but actually interact with real people on real hardware.\n\n### Beyond the lab\n\nFor me, research doesnâ€™t end as a paper. Iâ€™ve always been a challenger, constantly pushing myself to take ideas out of my head and into the real world. My curiosity usually starts with a 'why' or a 'it would be so cool to have that' and quickly turns into 'how can I build this?'. Whether Iâ€™m working on a robotic pipeline or a weekend side project, Iâ€™m driven by making things that are useful and grounded in the real world.\n\nI share these experiments and thoughts on **[Curious Intelligence](https://www.youtube.com/@loanbernat8145)**, my YouTube channel. Itâ€™s a place where I can be less formal, explore news ideas, and connect with people who are just as obsessed with the future of robotics as I am.\n\nIâ€™m always open to collaborating on projects that push the boundaries of multi-agent systems and robotic autonomy. Feel free to reach out.","site/content/about/index.md","9138628af079f6ab",{"html":84,"metadata":85},"\u003Cp>I am a \u003Cstrong>PhD Researcher in Robotics\u003C/strong> working on foundation models and long-horizon manipulation. Most of my days are spent thinking about language-conditioned tasks and how multi-agent systems can work together. I believe that true â€˜\u003Cem>intelligence\u003C/em>â€™ wonâ€™t emerge simply from scaling parameters, but through \u003Cstrong>System-level Intelligence\u003C/strong>. Because of this, I want to build robust, modular systems that donâ€™t just work in a paper, but actually interact with real people on real hardware.\u003C/p>\n\u003Ch3 id=\"beyond-the-lab\">\u003Ca class=\"heading-link\" aria-label=\"Link to section\" href=\"#beyond-the-lab\">#\u003C/a>Beyond the lab\u003C/h3>\n\u003Cp>For me, research doesnâ€™t end as a paper. Iâ€™ve always been a challenger, constantly pushing myself to take ideas out of my head and into the real world. My curiosity usually starts with a â€˜whyâ€™ or a â€˜it would be so cool to have thatâ€™ and quickly turns into â€˜how can I build this?â€™. Whether Iâ€™m working on a robotic pipeline or a weekend side project, Iâ€™m driven by making things that are useful and grounded in the real world.\u003C/p>\n\u003Cp>I share these experiments and thoughts on \u003Cstrong>\u003Ca href=\"https://www.youtube.com/@loanbernat8145\" rel=\"noopener noreferrer\" target=\"_blank\">Curious Intelligence\u003C/a>\u003C/strong>, my YouTube channel. Itâ€™s a place where I can be less formal, explore news ideas, and connect with people who are just as obsessed with the future of robotics as I am.\u003C/p>\n\u003Cp>Iâ€™m always open to collaborating on projects that push the boundaries of multi-agent systems and robotic autonomy. Feel free to reach out.\u003C/p>",{"headings":86,"localImagePaths":91,"remoteImagePaths":92,"frontmatter":93,"imagePaths":94},[87],{"depth":88,"slug":89,"text":90},3,"beyond-the-lab","#Beyond the lab",[],[],{"title":79},[],"research",["Map",97,98],"magma-gen",{"id":97,"data":99,"body":121,"filePath":122,"digest":123,"rendered":124},{"title":100,"description":101,"status":102,"tags":103,"collaborators":107,"link":110,"links":111,"order":118,"featured":119,"directLink":120},"MAGMA-GEN","Teaching robots to execute complex, multi-step tasks from natural language goals and scene observations without expert data.","under review",[104,105,106],"robotics","language","manipulation",[108,109],"SilÃ©ane","Gepetto Team (LAAS-CNRS)","https://example.com/lghm",[112,115],{"label":113,"url":114},"Paper","https://example.com/lghm-paper",{"label":116,"url":117},"Code","https://github.com/example/lghm",1,true,false,"> ðŸ—ï¸ Content on this page is a work in progress and will be updated as projects and publications are finalized.\n\nWe study how to bridge language, perception, and control to solve long-horizon tasks like \"prepare a workspace for soldering\" or \"pack a lunch.\" The system plans with compositional skills, adapts to failures, and executes in real robotic environments.","site/content/research/magma-gen.md","a49f0e527fdbb050",{"html":125,"metadata":126},"\u003Cblockquote>\n\u003Cp>ðŸ—ï¸ Content on this page is a work in progress and will be updated as projects and publications are finalized.\u003C/p>\n\u003C/blockquote>\n\u003Cp>We study how to bridge language, perception, and control to solve long-horizon tasks like â€œprepare a workspace for solderingâ€ or â€œpack a lunch.â€ The system plans with compositional skills, adapts to failures, and executes in real robotic environments.\u003C/p>",{"headings":127,"localImagePaths":128,"remoteImagePaths":129,"frontmatter":130,"imagePaths":136},[],[],[],{"title":100,"description":101,"status":102,"tags":131,"collaborators":132,"link":110,"links":133,"order":118,"featured":119,"directLink":120},[104,105,106],[108,109],[134,135],{"label":113,"url":114},{"label":116,"url":117},[]]