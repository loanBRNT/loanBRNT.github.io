[
  ["Map", 1, 2, 9, 10, 51, 52, 76, 77, 174, 175],
  "meta::meta",
  ["Map", 3, 4, 5, 6, 7, 8],
  "astro-version",
  "5.17.1",
  "content-config-digest",
  "4777298faf55afed",
  "astro-config-digest",
  "{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://github.com/loanBRNT\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"always\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{\"light\":\"min-light\",\"dark\":\"catppuccin-frappe\"},\"defaultColor\":false,\"wrap\":true,\"transformers\":[{\"name\":\"@shikijs/transformers:notation-highlight\"},{\"name\":\"@shikijs/transformers:notation-highlight-word\"},{\"name\":\"@shikijs/transformers:notation-diff\"}]},\"remarkPlugins\":[null],\"rehypePlugins\":[null,[null,{\"behavior\":\"prepend\",\"properties\":{\"className\":[\"heading-link\"],\"ariaLabel\":\"Link to section\"},\"content\":{\"type\":\"text\",\"value\":\"#\"}}],[null,{\"target\":\"_blank\",\"rel\":[\"noopener\",\"noreferrer\"]}]],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false,\"svgo\":false},\"legacy\":{\"collections\":false}}",
  "research",
  ["Map", 11, 12],
  "magma-gen",
  {
    "id": 11,
    "data": 13,
    "body": 35,
    "filePath": 36,
    "digest": 37,
    "rendered": 38
  },
  {
    "title": 14,
    "description": 15,
    "status": 16,
    "tags": 17,
    "collaborators": 21,
    "link": 24,
    "links": 25,
    "order": 32,
    "featured": 33,
    "directLink": 34
  },
  "MAGMA-GEN",
  "Teaching robots to execute complex, multi-step tasks from natural language goals and scene observations without expert data.",
  "under review",
  [18, 19, 20],
  "robotics",
  "language",
  "manipulation",
  [22, 23],
  "SilÃ©ane",
  "Gepetto Team (LAAS-CNRS)",
  "https://example.com/lghm",
  [26, 29],
  { "label": 27, "url": 28 },
  "Paper",
  "https://example.com/lghm-paper",
  { "label": 30, "url": 31 },
  "Code",
  "https://github.com/example/lghm",
  1,
  true,
  false,
  "> ðŸ—ï¸ Content on this page is a work in progress and will be updated as projects and publications are finalized.\n\nWe study how to bridge language, perception, and control to solve long-horizon tasks like \"prepare a workspace for soldering\" or \"pack a lunch.\" The system plans with compositional skills, adapts to failures, and executes in real robotic environments.",
  "site/content/research/magma-gen.md",
  "a49f0e527fdbb050",
  { "html": 39, "metadata": 40 },
  "\u003Cblockquote>\n\u003Cp>ðŸ—ï¸ Content on this page is a work in progress and will be updated as projects and publications are finalized.\u003C/p>\n\u003C/blockquote>\n\u003Cp>We study how to bridge language, perception, and control to solve long-horizon tasks like â€œprepare a workspace for solderingâ€ or â€œpack a lunch.â€ The system plans with compositional skills, adapts to failures, and executes in real robotic environments.\u003C/p>",
  {
    "headings": 41,
    "localImagePaths": 42,
    "remoteImagePaths": 43,
    "frontmatter": 44,
    "imagePaths": 50
  },
  [],
  [],
  [],
  {
    "title": 14,
    "description": 15,
    "status": 16,
    "tags": 45,
    "collaborators": 46,
    "link": 24,
    "links": 47,
    "order": 32,
    "featured": 33,
    "directLink": 34
  },
  [18, 19, 20],
  [22, 23],
  [48, 49],
  { "label": 27, "url": 28 },
  { "label": 30, "url": 31 },
  [],
  "publicationsPage",
  ["Map", 53, 54],
  "index",
  {
    "id": 53,
    "data": 55,
    "body": 58,
    "filePath": 59,
    "digest": 60,
    "rendered": 61
  },
  { "title": 56, "description": 57 },
  "Publications",
  "Journal papers, conference papers, and preprints.",
  "## Journal & Conference Papers\n\n*not yet*\n\n## Workshops & Preprints\n\n- **Addressing Long-Horizon failure in Language-Grounded Robotic via Structured Interaction**\n\n  *L. Bernat, A. Herbulot, M. Grard, F. Lamiraux*\n\n  Submitted to RSS 2026\n  \n  \u003Ca class=\"pub-button\" href=\"/research/magma-gen\">Related project\u003C/a>",
  "site/content/publications/index.md",
  "1d1c76a0cb5ab578",
  { "html": 62, "metadata": 63 },
  "\u003Ch2 id=\"journal--conference-papers\">\u003Ca class=\"heading-link\" aria-label=\"Link to section\" href=\"#journal--conference-papers\">#\u003C/a>Journal &#x26; Conference Papers\u003C/h2>\n\u003Cp>\u003Cem>not yet\u003C/em>\u003C/p>\n\u003Ch2 id=\"workshops--preprints\">\u003Ca class=\"heading-link\" aria-label=\"Link to section\" href=\"#workshops--preprints\">#\u003C/a>Workshops &#x26; Preprints\u003C/h2>\n\u003Cul>\n\u003Cli>\n\u003Cp>\u003Cstrong>Addressing Long-Horizon failure in Language-Grounded Robotic via Structured Interaction\u003C/strong>\u003C/p>\n\u003Cp>\u003Cem>L. Bernat, A. Herbulot, M. Grard, F. Lamiraux\u003C/em>\u003C/p>\n\u003Cp>Submitted to RSS 2026\u003C/p>\n\u003Cp>\u003Ca class=\"pub-button\" href=\"/research/magma-gen\">Related project\u003C/a>\u003C/p>\n\u003C/li>\n\u003C/ul>",
  {
    "headings": 64,
    "localImagePaths": 72,
    "remoteImagePaths": 73,
    "frontmatter": 74,
    "imagePaths": 75
  },
  [65, 69],
  { "depth": 66, "slug": 67, "text": 68 },
  2,
  "journal--conference-papers",
  "#Journal & Conference Papers",
  { "depth": 66, "slug": 70, "text": 71 },
  "workshops--preprints",
  "#Workshops & Preprints",
  [],
  [],
  { "title": 56, "description": 57 },
  [],
  "projects",
  ["Map", 78, 79, 114, 115, 146, 147],
  "sawyer-bartender",
  {
    "id": 78,
    "data": 80,
    "body": 92,
    "filePath": 93,
    "digest": 94,
    "rendered": 95
  },
  {
    "title": 81,
    "description": 82,
    "video": 83,
    "github": 84,
    "tags": 85,
    "types": 90,
    "order": 66
  },
  "Building a Robot Bartender",
  "How I turned a Sawyer manipulator into a bartender and built a no-code system for students to control it",
  "https://youtu.be/rq2TTOWhma8?si=0zAll2QjgnIzqZab",
  "https://github.com/loanBRNT/sawyer_vision_bartender",
  [86, 87, 88, 89],
  "Python",
  "ROS/ROS2",
  "Detection",
  "Control",
  [91],
  "internship",
  "This project was developed during a research internship at Ostfalia University of Applied Sciences (Germany).\nThe objective was to design a complete robotic bartender system using a [Sawyer manipulator](https://www.rethinkrobotics.com/sawyer), integrated with a [Pepper robot](https://www.aldebaran.com/en/pepper) acting as a waiter. My focus was entirely on the Sawyer application.\n\n## System Overview\n\nI treated this as a full-stack robotics problem:\n\n- **Perception:** I trained and integrated YOLOv5 to identify bottles and cups. While being reliable under the shifting lights of a lab environment.\n- **Manipulation:** I designed the grasp sequencing to handle different bottle shapes and ensure a steady pour without \"spilling\".\n- **The Bridge (No-Code):** I built a Node-RED interface that abstracted the ROS complexity. It allowed Masterâ€™s students to \"program\" the robot for their own experiments.\n\nIt was one of my first autonomous project on a real robot. Supervisors let me explore any solution that I want and I really faced the curse of any real robots application: race conditions ðŸ«¡ and timing issues. \n\n## Technical Challenges & Design Choices\n\n- **Robust perception**: bottle and cup detection needed to be reliable under varying lighting and clutter.\n- **Grasp sequencing**: handling bottles safely while ensuring stable pouring.\n- **Human-in-the-loop coordination**: synchronizing actions with Pepper or Humans required explicit confirmation handling.\n- **Usability**: the Node-RED interface had to abstract ROS complexity while remaining expressive for students.\n\n## Outcome\n\nThe final system was able to autonomously execute drink-serving sequences, from order recognition to cup delivery, while remaining configurable through a no-code interface.\n\nThis project demonstrates my early experience with:\n- full-stack robotic systems\n- perception-driven manipulation\n- ROS-based orchestration\n- designing interfaces for non-roboticists",
  "site/content/projects/sawyer-bartender.md",
  "6708c36bfcbfe853",
  { "html": 96, "metadata": 97 },
  "\u003Cp>This project was developed during a research internship at Ostfalia University of Applied Sciences (Germany).\nThe objective was to design a complete robotic bartender system using a \u003Ca href=\"https://www.rethinkrobotics.com/sawyer\" rel=\"noopener noreferrer\" target=\"_blank\">Sawyer manipulator\u003C/a>, integrated with a \u003Ca href=\"https://www.aldebaran.com/en/pepper\" rel=\"noopener noreferrer\" target=\"_blank\">Pepper robot\u003C/a> acting as a waiter. My focus was entirely on the Sawyer application.\u003C/p>\n\u003Ch2 id=\"system-overview\">\u003Ca class=\"heading-link\" aria-label=\"Link to section\" href=\"#system-overview\">#\u003C/a>System Overview\u003C/h2>\n\u003Cp>I treated this as a full-stack robotics problem:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Perception:\u003C/strong> I trained and integrated YOLOv5 to identify bottles and cups. While being reliable under the shifting lights of a lab environment.\u003C/li>\n\u003Cli>\u003Cstrong>Manipulation:\u003C/strong> I designed the grasp sequencing to handle different bottle shapes and ensure a steady pour without â€œspillingâ€.\u003C/li>\n\u003Cli>\u003Cstrong>The Bridge (No-Code):\u003C/strong> I built a Node-RED interface that abstracted the ROS complexity. It allowed Masterâ€™s students to â€œprogramâ€ the robot for their own experiments.\u003C/li>\n\u003C/ul>\n\u003Cp>It was one of my first autonomous project on a real robot. Supervisors let me explore any solution that I want and I really faced the curse of any real robots application: race conditions ðŸ«¡ and timing issues.\u003C/p>\n\u003Ch2 id=\"technical-challenges--design-choices\">\u003Ca class=\"heading-link\" aria-label=\"Link to section\" href=\"#technical-challenges--design-choices\">#\u003C/a>Technical Challenges &#x26; Design Choices\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Robust perception\u003C/strong>: bottle and cup detection needed to be reliable under varying lighting and clutter.\u003C/li>\n\u003Cli>\u003Cstrong>Grasp sequencing\u003C/strong>: handling bottles safely while ensuring stable pouring.\u003C/li>\n\u003Cli>\u003Cstrong>Human-in-the-loop coordination\u003C/strong>: synchronizing actions with Pepper or Humans required explicit confirmation handling.\u003C/li>\n\u003Cli>\u003Cstrong>Usability\u003C/strong>: the Node-RED interface had to abstract ROS complexity while remaining expressive for students.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"outcome\">\u003Ca class=\"heading-link\" aria-label=\"Link to section\" href=\"#outcome\">#\u003C/a>Outcome\u003C/h2>\n\u003Cp>The final system was able to autonomously execute drink-serving sequences, from order recognition to cup delivery, while remaining configurable through a no-code interface.\u003C/p>\n\u003Cp>This project demonstrates my early experience with:\u003C/p>\n\u003Cul>\n\u003Cli>full-stack robotic systems\u003C/li>\n\u003Cli>perception-driven manipulation\u003C/li>\n\u003Cli>ROS-based orchestration\u003C/li>\n\u003Cli>designing interfaces for non-roboticists\u003C/li>\n\u003C/ul>",
  {
    "headings": 98,
    "localImagePaths": 108,
    "remoteImagePaths": 109,
    "frontmatter": 110,
    "imagePaths": 113
  },
  [99, 102, 105],
  { "depth": 66, "slug": 100, "text": 101 },
  "system-overview",
  "#System Overview",
  { "depth": 66, "slug": 103, "text": 104 },
  "technical-challenges--design-choices",
  "#Technical Challenges & Design Choices",
  { "depth": 66, "slug": 106, "text": 107 },
  "outcome",
  "#Outcome",
  [],
  [],
  {
    "title": 81,
    "description": 82,
    "video": 83,
    "github": 84,
    "tags": 111,
    "types": 112,
    "order": 66
  },
  [86, 87, 88, 89],
  [91],
  [],
  "hospital-4-0",
  {
    "id": 114,
    "data": 116,
    "body": 129,
    "filePath": 130,
    "digest": 131,
    "rendered": 132
  },
  {
    "title": 117,
    "description": 118,
    "video": 119,
    "github": 120,
    "tags": 121,
    "types": 125,
    "order": 128
  },
  "Hospital 4.0",
  " Simulate a robotic application to assist nurses in a hospital in responding to non-critical patient needs. ",
  "https://youtu.be/7Lj5-pO2FFQ?si=z12Duxx_V_gcuzS8",
  "https://github.com/loanBRNT/delivery_packing_python",
  [86, 122, 123, 124],
  "IsaacSim",
  "Multi-Robot",
  "Language",
  [126, 127],
  "hackathon",
  "open-source",
  0,
  "This project was initiated during a robotics hackathon organized by [REVEL](https://revelstudios.io/) and [LycheeAI](https://lycheeai-hub.com/), with the objective of building a useful, manipulation-oriented robotic application.\n\nThe scenario focuses on hospital environments, where nursing staff are often burdened with non-critical logistical tasks. The goal of the project is to explore how robotic systems could assist by handling simple requestsâ€”such as delivering food, drinks, or everyday itemsâ€”allowing nurses to focus on higher-value medical care.\n\nThe system is designed around natural language interaction, enabling patients to express requests in a flexible way. These requests are then interpreted and executed by one or more robotic agents within a simulated hospital environment.  \nThis fully working prototype won **$600** at the hackathon and can be reused or modified freely for other projects.\n\n## Key Technical Challenges\n\n- Designing a lightweight multi-robot coordination state machine capable of synchronizing a mobile base and a manipulator under tight hackathon time constraints.\n- Translating open-ended natural language requests into structured robotic actions without relying on cloud inference, enabling execution on a standard laptop.\n- Managing asynchronous execution and task handoff between robots to minimize idle time during delivery.\n- Integrating perception, language interpretation, and control into a single coherent pipeline within Isaac Sim.\n\n## System Overview\n\nThe project demonstrates two core components, both integrated into Isaac Sim:\n\n- **Robot Collaboration:** A coordination program that synchronizes a mobile robot and a manipulator using a state machine, allowing them to work together efficiently to deliver objects to patients.\n- **Language Control:** A lightweight pipeline that converts patient instructions expressed in natural language into executable robotic actions, designed to remain efficient enough to run on a laptop.",
  "site/content/projects/hospital-4-0.md",
  "adccb970051ad6fe",
  { "html": 133, "metadata": 134 },
  "\u003Cp>This project was initiated during a robotics hackathon organized by \u003Ca href=\"https://revelstudios.io/\" rel=\"noopener noreferrer\" target=\"_blank\">REVEL\u003C/a> and \u003Ca href=\"https://lycheeai-hub.com/\" rel=\"noopener noreferrer\" target=\"_blank\">LycheeAI\u003C/a>, with the objective of building a useful, manipulation-oriented robotic application.\u003C/p>\n\u003Cp>The scenario focuses on hospital environments, where nursing staff are often burdened with non-critical logistical tasks. The goal of the project is to explore how robotic systems could assist by handling simple requestsâ€”such as delivering food, drinks, or everyday itemsâ€”allowing nurses to focus on higher-value medical care.\u003C/p>\n\u003Cp>The system is designed around natural language interaction, enabling patients to express requests in a flexible way. These requests are then interpreted and executed by one or more robotic agents within a simulated hospital environment.\u003Cbr>\nThis fully working prototype won \u003Cstrong>$600\u003C/strong> at the hackathon and can be reused or modified freely for other projects.\u003C/p>\n\u003Ch2 id=\"key-technical-challenges\">\u003Ca class=\"heading-link\" aria-label=\"Link to section\" href=\"#key-technical-challenges\">#\u003C/a>Key Technical Challenges\u003C/h2>\n\u003Cul>\n\u003Cli>Designing a lightweight multi-robot coordination state machine capable of synchronizing a mobile base and a manipulator under tight hackathon time constraints.\u003C/li>\n\u003Cli>Translating open-ended natural language requests into structured robotic actions without relying on cloud inference, enabling execution on a standard laptop.\u003C/li>\n\u003Cli>Managing asynchronous execution and task handoff between robots to minimize idle time during delivery.\u003C/li>\n\u003Cli>Integrating perception, language interpretation, and control into a single coherent pipeline within Isaac Sim.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"system-overview\">\u003Ca class=\"heading-link\" aria-label=\"Link to section\" href=\"#system-overview\">#\u003C/a>System Overview\u003C/h2>\n\u003Cp>The project demonstrates two core components, both integrated into Isaac Sim:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Robot Collaboration:\u003C/strong> A coordination program that synchronizes a mobile robot and a manipulator using a state machine, allowing them to work together efficiently to deliver objects to patients.\u003C/li>\n\u003Cli>\u003Cstrong>Language Control:\u003C/strong> A lightweight pipeline that converts patient instructions expressed in natural language into executable robotic actions, designed to remain efficient enough to run on a laptop.\u003C/li>\n\u003C/ul>",
  {
    "headings": 135,
    "localImagePaths": 140,
    "remoteImagePaths": 141,
    "frontmatter": 142,
    "imagePaths": 145
  },
  [136, 139],
  { "depth": 66, "slug": 137, "text": 138 },
  "key-technical-challenges",
  "#Key Technical Challenges",
  { "depth": 66, "slug": 100, "text": 101 },
  [],
  [],
  {
    "title": 117,
    "description": 118,
    "video": 119,
    "github": 120,
    "tags": 143,
    "types": 144,
    "order": 128
  },
  [86, 122, 123, 124],
  [126, 127],
  [],
  "novel-view-synthesis",
  {
    "id": 146,
    "data": 148,
    "body": 154,
    "filePath": 155,
    "digest": 156,
    "rendered": 157
  },
  { "title": 149, "description": 150, "tags": 151, "types": 153, "order": 66 },
  "Novel View Synthesis",
  "Research internship on reducing annotation requirements for visual object classification using generative models.",
  [86, 152],
  "Generative AI",
  [91],
  "This project was conducted during a research internship at [SilÃ¨ane](https://www.sileane.com/en/).  \nDue to industrial confidentiality constraints, the code and datasets are not open-source.\n\nThe objective was to investigate whether **generative novel-view synthesis** could reduce the amount of human-annotated data required to train robust object classification models in industrial settings, where annotation is costly and time-consuming.\n\n## Problem Setting\n\nIn industrial perception pipelines, object classifiers often rely on large annotated datasets to handle variations in viewpoint, lighting, and appearance. However, collecting such datasets is expensive and does not scale well to new objects or frequent changes in production.\n\nThe core question addressed in this internship was:\n\n> *Can a model trained on a very small number of annotated images achieve strong generalization by leveraging synthetic visual variations generated by a diffusion-based model?*\n\n## System Overview\n\nThe proposed pipeline takes as input a **single annotated image of an object** and generates multiple realistic variations by modifying:\n- viewpoint,\n- lighting conditions,\n- texture appearance.\n\nDiffusion models were used to generate these variations while preserving object identity, with the goal of improving robustness to visual distribution shifts during training.\n\nThe augmented dataset was then used to train an object classification network, and its performance was compared against a baseline trained only on human-annotated data.\n\n## Technical Challenges & Design Choices\n\n- Controlling generative diversity while maintaining object identity to avoid label noise.\n- Preventing distribution drift between synthetic and real data.\n- Balancing augmentation strength to improve generalization without overfitting to artifacts.\n- Designing an evaluation protocol suitable for low-data regimes.\n\n## Outcome\n\nThe final system was able to generate up to **46 distinct visual variations** from a single annotated image.\n\nExperiments showed that, with **only a dozen human-annotated images**, the classifier reached up to **98% accuracy** on the target object classification taskâ€”significantly outperforming models trained without synthetic augmentation.\n\nThis work demonstrated the practical potential of generative models as a data-efficient alternative to large-scale manual annotation in industrial vision pipelines.",
  "site/content/projects/novel-view-synthesis.md",
  "a54ad17527227421",
  { "html": 158, "metadata": 159 },
  "\u003Cp>This project was conducted during a research internship at \u003Ca href=\"https://www.sileane.com/en/\" rel=\"noopener noreferrer\" target=\"_blank\">SilÃ¨ane\u003C/a>.\u003Cbr>\nDue to industrial confidentiality constraints, the code and datasets are not open-source.\u003C/p>\n\u003Cp>The objective was to investigate whether \u003Cstrong>generative novel-view synthesis\u003C/strong> could reduce the amount of human-annotated data required to train robust object classification models in industrial settings, where annotation is costly and time-consuming.\u003C/p>\n\u003Ch2 id=\"problem-setting\">\u003Ca class=\"heading-link\" aria-label=\"Link to section\" href=\"#problem-setting\">#\u003C/a>Problem Setting\u003C/h2>\n\u003Cp>In industrial perception pipelines, object classifiers often rely on large annotated datasets to handle variations in viewpoint, lighting, and appearance. However, collecting such datasets is expensive and does not scale well to new objects or frequent changes in production.\u003C/p>\n\u003Cp>The core question addressed in this internship was:\u003C/p>\n\u003Cblockquote>\n\u003Cp>\u003Cem>Can a model trained on a very small number of annotated images achieve strong generalization by leveraging synthetic visual variations generated by a diffusion-based model?\u003C/em>\u003C/p>\n\u003C/blockquote>\n\u003Ch2 id=\"system-overview\">\u003Ca class=\"heading-link\" aria-label=\"Link to section\" href=\"#system-overview\">#\u003C/a>System Overview\u003C/h2>\n\u003Cp>The proposed pipeline takes as input a \u003Cstrong>single annotated image of an object\u003C/strong> and generates multiple realistic variations by modifying:\u003C/p>\n\u003Cul>\n\u003Cli>viewpoint,\u003C/li>\n\u003Cli>lighting conditions,\u003C/li>\n\u003Cli>texture appearance.\u003C/li>\n\u003C/ul>\n\u003Cp>Diffusion models were used to generate these variations while preserving object identity, with the goal of improving robustness to visual distribution shifts during training.\u003C/p>\n\u003Cp>The augmented dataset was then used to train an object classification network, and its performance was compared against a baseline trained only on human-annotated data.\u003C/p>\n\u003Ch2 id=\"technical-challenges--design-choices\">\u003Ca class=\"heading-link\" aria-label=\"Link to section\" href=\"#technical-challenges--design-choices\">#\u003C/a>Technical Challenges &#x26; Design Choices\u003C/h2>\n\u003Cul>\n\u003Cli>Controlling generative diversity while maintaining object identity to avoid label noise.\u003C/li>\n\u003Cli>Preventing distribution drift between synthetic and real data.\u003C/li>\n\u003Cli>Balancing augmentation strength to improve generalization without overfitting to artifacts.\u003C/li>\n\u003Cli>Designing an evaluation protocol suitable for low-data regimes.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"outcome\">\u003Ca class=\"heading-link\" aria-label=\"Link to section\" href=\"#outcome\">#\u003C/a>Outcome\u003C/h2>\n\u003Cp>The final system was able to generate up to \u003Cstrong>46 distinct visual variations\u003C/strong> from a single annotated image.\u003C/p>\n\u003Cp>Experiments showed that, with \u003Cstrong>only a dozen human-annotated images\u003C/strong>, the classifier reached up to \u003Cstrong>98% accuracy\u003C/strong> on the target object classification taskâ€”significantly outperforming models trained without synthetic augmentation.\u003C/p>\n\u003Cp>This work demonstrated the practical potential of generative models as a data-efficient alternative to large-scale manual annotation in industrial vision pipelines.\u003C/p>",
  {
    "headings": 160,
    "localImagePaths": 167,
    "remoteImagePaths": 168,
    "frontmatter": 169,
    "imagePaths": 173
  },
  [161, 164, 165, 166],
  { "depth": 66, "slug": 162, "text": 163 },
  "problem-setting",
  "#Problem Setting",
  { "depth": 66, "slug": 100, "text": 101 },
  { "depth": 66, "slug": 103, "text": 104 },
  { "depth": 66, "slug": 106, "text": 107 },
  [],
  [],
  {
    "title": 149,
    "date": 170,
    "description": 150,
    "tags": 171,
    "types": 172,
    "order": 66
  },
  "04.01.2024",
  [86, 152],
  [91],
  [],
  "about",
  ["Map", 53, 176],
  {
    "id": 53,
    "data": 177,
    "body": 179,
    "filePath": 180,
    "digest": 181,
    "rendered": 182
  },
  { "title": 178 },
  "Because Intelligence starts with curiosity",
  "I am a **PhD Researcher in Robotics** working on foundation models and long-horizon manipulation. Most of my days are spent thinking about language-conditioned tasks and how multi-agent systems can work together. I believe that true '*intelligence*' won't emerge simply from scaling parameters, but through **System-level Intelligence**. Because of this, I want to build robust, modular systems that don't just work in a paper, but actually interact with real people on real hardware.\n\n### Beyond the lab\n\nFor me, research doesnâ€™t end as a paper. Iâ€™ve always been a challenger, constantly pushing myself to take ideas out of my head and into the real world. My curiosity usually starts with a 'why' or a 'it would be so cool to have that' and quickly turns into 'how can I build this?'. Whether Iâ€™m working on a robotic pipeline or a weekend side project, Iâ€™m driven by making things that are useful and grounded in the real world.\n\nI share these experiments and thoughts on **[Curious Intelligence](https://www.youtube.com/@loanbernat8145)**, my YouTube channel. Itâ€™s a place where I can be less formal, explore news ideas, and connect with people who are just as obsessed with the future of robotics as I am.\n\nIâ€™m always open to collaborating on projects that push the boundaries of multi-agent systems and robotic autonomy. Feel free to reach out.",
  "site/content/about/index.md",
  "9138628af079f6ab",
  { "html": 183, "metadata": 184 },
  "\u003Cp>I am a \u003Cstrong>PhD Researcher in Robotics\u003C/strong> working on foundation models and long-horizon manipulation. Most of my days are spent thinking about language-conditioned tasks and how multi-agent systems can work together. I believe that true â€˜\u003Cem>intelligence\u003C/em>â€™ wonâ€™t emerge simply from scaling parameters, but through \u003Cstrong>System-level Intelligence\u003C/strong>. Because of this, I want to build robust, modular systems that donâ€™t just work in a paper, but actually interact with real people on real hardware.\u003C/p>\n\u003Ch3 id=\"beyond-the-lab\">\u003Ca class=\"heading-link\" aria-label=\"Link to section\" href=\"#beyond-the-lab\">#\u003C/a>Beyond the lab\u003C/h3>\n\u003Cp>For me, research doesnâ€™t end as a paper. Iâ€™ve always been a challenger, constantly pushing myself to take ideas out of my head and into the real world. My curiosity usually starts with a â€˜whyâ€™ or a â€˜it would be so cool to have thatâ€™ and quickly turns into â€˜how can I build this?â€™. Whether Iâ€™m working on a robotic pipeline or a weekend side project, Iâ€™m driven by making things that are useful and grounded in the real world.\u003C/p>\n\u003Cp>I share these experiments and thoughts on \u003Cstrong>\u003Ca href=\"https://www.youtube.com/@loanbernat8145\" rel=\"noopener noreferrer\" target=\"_blank\">Curious Intelligence\u003C/a>\u003C/strong>, my YouTube channel. Itâ€™s a place where I can be less formal, explore news ideas, and connect with people who are just as obsessed with the future of robotics as I am.\u003C/p>\n\u003Cp>Iâ€™m always open to collaborating on projects that push the boundaries of multi-agent systems and robotic autonomy. Feel free to reach out.\u003C/p>",
  {
    "headings": 185,
    "localImagePaths": 190,
    "remoteImagePaths": 191,
    "frontmatter": 192,
    "imagePaths": 193
  },
  [186],
  { "depth": 187, "slug": 188, "text": 189 },
  3,
  "beyond-the-lab",
  "#Beyond the lab",
  [],
  [],
  { "title": 178 },
  []
]
