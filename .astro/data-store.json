[
  ["Map", 1, 2, 9, 10, 51, 52, 76, 77, 143, 144],
  "meta::meta",
  ["Map", 3, 4, 5, 6, 7, 8],
  "astro-version",
  "5.17.1",
  "content-config-digest",
  "4777298faf55afed",
  "astro-config-digest",
  "{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://github.com/loanBRNT\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"always\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{\"light\":\"min-light\",\"dark\":\"catppuccin-frappe\"},\"defaultColor\":false,\"wrap\":true,\"transformers\":[{\"name\":\"@shikijs/transformers:notation-highlight\"},{\"name\":\"@shikijs/transformers:notation-highlight-word\"},{\"name\":\"@shikijs/transformers:notation-diff\"}]},\"remarkPlugins\":[null],\"rehypePlugins\":[null,[null,{\"behavior\":\"prepend\",\"properties\":{\"className\":[\"heading-link\"],\"ariaLabel\":\"Link to section\"},\"content\":{\"type\":\"text\",\"value\":\"#\"}}],[null,{\"target\":\"_blank\",\"rel\":[\"noopener\",\"noreferrer\"]}]],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false,\"svgo\":false},\"legacy\":{\"collections\":false}}",
  "research",
  ["Map", 11, 12],
  "magma-gen",
  {
    "id": 11,
    "data": 13,
    "body": 35,
    "filePath": 36,
    "digest": 37,
    "rendered": 38
  },
  {
    "title": 14,
    "description": 15,
    "status": 16,
    "tags": 17,
    "collaborators": 21,
    "link": 24,
    "links": 25,
    "order": 32,
    "featured": 33,
    "directLink": 34
  },
  "MAGMA-GEN",
  "Teaching robots to execute complex, multi-step tasks from natural language goals and scene observations without expert data.",
  "under review",
  [18, 19, 20],
  "robotics",
  "language",
  "manipulation",
  [22, 23],
  "SilÃ©ane",
  "Gepetto Team (LAAS-CNRS)",
  "https://example.com/lghm",
  [26, 29],
  { "label": 27, "url": 28 },
  "Paper",
  "https://example.com/lghm-paper",
  { "label": 30, "url": 31 },
  "Code",
  "https://github.com/example/lghm",
  1,
  true,
  false,
  "> ðŸ—ï¸ Content on this page is a work in progress and will be updated as projects and publications are finalized.\n\nWe study how to bridge language, perception, and control to solve long-horizon tasks like \"prepare a workspace for soldering\" or \"pack a lunch.\" The system plans with compositional skills, adapts to failures, and executes in real robotic environments.",
  "site/content/research/magma-gen.md",
  "a49f0e527fdbb050",
  { "html": 39, "metadata": 40 },
  "\u003Cblockquote>\n\u003Cp>ðŸ—ï¸ Content on this page is a work in progress and will be updated as projects and publications are finalized.\u003C/p>\n\u003C/blockquote>\n\u003Cp>We study how to bridge language, perception, and control to solve long-horizon tasks like â€œprepare a workspace for solderingâ€ or â€œpack a lunch.â€ The system plans with compositional skills, adapts to failures, and executes in real robotic environments.\u003C/p>",
  {
    "headings": 41,
    "localImagePaths": 42,
    "remoteImagePaths": 43,
    "frontmatter": 44,
    "imagePaths": 50
  },
  [],
  [],
  [],
  {
    "title": 14,
    "description": 15,
    "status": 16,
    "tags": 45,
    "collaborators": 46,
    "link": 24,
    "links": 47,
    "order": 32,
    "featured": 33,
    "directLink": 34
  },
  [18, 19, 20],
  [22, 23],
  [48, 49],
  { "label": 27, "url": 28 },
  { "label": 30, "url": 31 },
  [],
  "publicationsPage",
  ["Map", 53, 54],
  "index",
  {
    "id": 53,
    "data": 55,
    "body": 58,
    "filePath": 59,
    "digest": 60,
    "rendered": 61
  },
  { "title": 56, "description": 57 },
  "Publications",
  "Journal papers, conference papers, and preprints.",
  "## Journal & Conference Papers\n\n*not yet*\n\n## Workshops & Preprints\n\n- **Addressing Long-Horizon failure in Language-Grounded Robotic via Structured Interaction**\n\n  *L. Bernat, A. Herbulot, M. Grard, F. Lamiraux*\n\n  Submitted to RSS 2026\n  \n  \u003Ca class=\"pub-button\" href=\"/research/magma-gen\">Related project\u003C/a>",
  "site/content/publications/index.md",
  "1d1c76a0cb5ab578",
  { "html": 62, "metadata": 63 },
  "\u003Ch2 id=\"journal--conference-papers\">\u003Ca class=\"heading-link\" aria-label=\"Link to section\" href=\"#journal--conference-papers\">#\u003C/a>Journal &#x26; Conference Papers\u003C/h2>\n\u003Cp>\u003Cem>not yet\u003C/em>\u003C/p>\n\u003Ch2 id=\"workshops--preprints\">\u003Ca class=\"heading-link\" aria-label=\"Link to section\" href=\"#workshops--preprints\">#\u003C/a>Workshops &#x26; Preprints\u003C/h2>\n\u003Cul>\n\u003Cli>\n\u003Cp>\u003Cstrong>Addressing Long-Horizon failure in Language-Grounded Robotic via Structured Interaction\u003C/strong>\u003C/p>\n\u003Cp>\u003Cem>L. Bernat, A. Herbulot, M. Grard, F. Lamiraux\u003C/em>\u003C/p>\n\u003Cp>Submitted to RSS 2026\u003C/p>\n\u003Cp>\u003Ca class=\"pub-button\" href=\"/research/magma-gen\">Related project\u003C/a>\u003C/p>\n\u003C/li>\n\u003C/ul>",
  {
    "headings": 64,
    "localImagePaths": 72,
    "remoteImagePaths": 73,
    "frontmatter": 74,
    "imagePaths": 75
  },
  [65, 69],
  { "depth": 66, "slug": 67, "text": 68 },
  2,
  "journal--conference-papers",
  "#Journal & Conference Papers",
  { "depth": 66, "slug": 70, "text": 71 },
  "workshops--preprints",
  "#Workshops & Preprints",
  [],
  [],
  { "title": 56, "description": 57 },
  [],
  "projects",
  ["Map", 78, 79, 114, 115],
  "sawyer-bartender",
  {
    "id": 78,
    "data": 80,
    "body": 92,
    "filePath": 93,
    "digest": 94,
    "rendered": 95
  },
  {
    "title": 81,
    "description": 82,
    "video": 83,
    "github": 84,
    "tags": 85,
    "types": 90,
    "order": 66
  },
  "Building a Robot Bartender",
  "How I turned a Sawyer manipulator into a bartender and built a no-code system for others to control it",
  "https://youtu.be/rq2TTOWhma8?si=0zAll2QjgnIzqZab",
  "https://github.com/loanBRNT/sawyer_vision_bartender",
  [86, 87, 88, 89],
  "Python",
  "ROS/ROS2",
  "Detection",
  "Control",
  [91],
  "internship",
  "This project was developed during a research internship at Ostfalia University of Applied Sciences (Germany).\nThe objective was to design a complete robotic bartender system using a [Sawyer manipulator](https://www.rethinkrobotics.com/sawyer), integrated with a [Pepper robot](https://www.aldebaran.com/en/pepper) acting as a waiter. My focus was entirely on the Sawyer application.\n\n## System Overview\n\nI treated this as a full-stack robotics problem:\n\n- **Perception:** I trained and integrated YOLOv5 to identify bottles and cups. While being reliable under the shifting lights of a lab environment.\n- **Manipulation:** I designed the grasp sequencing to handle different bottle shapes and ensure a steady pour without \"spilling\".\n- **The Bridge (No-Code):** I built a Node-RED interface that abstracted the ROS complexity. It allowed Masterâ€™s students to \"program\" the robot for their own experiments.\n\nIt was one of my first autonomous project on a real robot. Supervisors let me explore any solution that I want and I really faced the curse of any real robots application: race conditions ðŸ«¡ and timing issues. \n\n## Technical Challenges & Design Choices\n\n- **Robust perception**: bottle and cup detection needed to be reliable under varying lighting and clutter.\n- **Grasp sequencing**: handling bottles safely while ensuring stable pouring.\n- **Human-in-the-loop coordination**: synchronizing actions with Pepper or Humans required explicit confirmation handling.\n- **Usability**: the Node-RED interface had to abstract ROS complexity while remaining expressive for students.\n\n## Outcome\n\nThe final system was able to autonomously execute drink-serving sequences, from order recognition to cup delivery, while remaining configurable through a no-code interface.\n\nThis project demonstrates my early experience with:\n- full-stack robotic systems\n- perception-driven manipulation\n- ROS-based orchestration\n- designing interfaces for non-roboticists",
  "site/content/projects/sawyer-bartender.md",
  "a5b23325e3cf9cca",
  { "html": 96, "metadata": 97 },
  "\u003Cp>This project was developed during a research internship at Ostfalia University of Applied Sciences (Germany).\nThe objective was to design a complete robotic bartender system using a \u003Ca href=\"https://www.rethinkrobotics.com/sawyer\" rel=\"noopener noreferrer\" target=\"_blank\">Sawyer manipulator\u003C/a>, integrated with a \u003Ca href=\"https://www.aldebaran.com/en/pepper\" rel=\"noopener noreferrer\" target=\"_blank\">Pepper robot\u003C/a> acting as a waiter. My focus was entirely on the Sawyer application.\u003C/p>\n\u003Ch2 id=\"system-overview\">\u003Ca class=\"heading-link\" aria-label=\"Link to section\" href=\"#system-overview\">#\u003C/a>System Overview\u003C/h2>\n\u003Cp>I treated this as a full-stack robotics problem:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Perception:\u003C/strong> I trained and integrated YOLOv5 to identify bottles and cups. While being reliable under the shifting lights of a lab environment.\u003C/li>\n\u003Cli>\u003Cstrong>Manipulation:\u003C/strong> I designed the grasp sequencing to handle different bottle shapes and ensure a steady pour without â€œspillingâ€.\u003C/li>\n\u003Cli>\u003Cstrong>The Bridge (No-Code):\u003C/strong> I built a Node-RED interface that abstracted the ROS complexity. It allowed Masterâ€™s students to â€œprogramâ€ the robot for their own experiments.\u003C/li>\n\u003C/ul>\n\u003Cp>It was one of my first autonomous project on a real robot. Supervisors let me explore any solution that I want and I really faced the curse of any real robots application: race conditions ðŸ«¡ and timing issues.\u003C/p>\n\u003Ch2 id=\"technical-challenges--design-choices\">\u003Ca class=\"heading-link\" aria-label=\"Link to section\" href=\"#technical-challenges--design-choices\">#\u003C/a>Technical Challenges &#x26; Design Choices\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Robust perception\u003C/strong>: bottle and cup detection needed to be reliable under varying lighting and clutter.\u003C/li>\n\u003Cli>\u003Cstrong>Grasp sequencing\u003C/strong>: handling bottles safely while ensuring stable pouring.\u003C/li>\n\u003Cli>\u003Cstrong>Human-in-the-loop coordination\u003C/strong>: synchronizing actions with Pepper or Humans required explicit confirmation handling.\u003C/li>\n\u003Cli>\u003Cstrong>Usability\u003C/strong>: the Node-RED interface had to abstract ROS complexity while remaining expressive for students.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"outcome\">\u003Ca class=\"heading-link\" aria-label=\"Link to section\" href=\"#outcome\">#\u003C/a>Outcome\u003C/h2>\n\u003Cp>The final system was able to autonomously execute drink-serving sequences, from order recognition to cup delivery, while remaining configurable through a no-code interface.\u003C/p>\n\u003Cp>This project demonstrates my early experience with:\u003C/p>\n\u003Cul>\n\u003Cli>full-stack robotic systems\u003C/li>\n\u003Cli>perception-driven manipulation\u003C/li>\n\u003Cli>ROS-based orchestration\u003C/li>\n\u003Cli>designing interfaces for non-roboticists\u003C/li>\n\u003C/ul>",
  {
    "headings": 98,
    "localImagePaths": 108,
    "remoteImagePaths": 109,
    "frontmatter": 110,
    "imagePaths": 113
  },
  [99, 102, 105],
  { "depth": 66, "slug": 100, "text": 101 },
  "system-overview",
  "#System Overview",
  { "depth": 66, "slug": 103, "text": 104 },
  "technical-challenges--design-choices",
  "#Technical Challenges & Design Choices",
  { "depth": 66, "slug": 106, "text": 107 },
  "outcome",
  "#Outcome",
  [],
  [],
  {
    "title": 81,
    "description": 82,
    "video": 83,
    "github": 84,
    "tags": 111,
    "types": 112,
    "order": 66
  },
  [86, 87, 88, 89],
  [91],
  [],
  "hospital-4-0",
  {
    "id": 114,
    "data": 116,
    "body": 129,
    "filePath": 130,
    "digest": 131,
    "rendered": 132
  },
  {
    "title": 117,
    "description": 118,
    "video": 119,
    "github": 120,
    "tags": 121,
    "types": 125,
    "order": 128
  },
  "Hospital 4.0",
  "Simulate a robotic application to help nurse in a hospital to brings non-critics needs for patient",
  "https://youtu.be/7Lj5-pO2FFQ?si=z12Duxx_V_gcuzS8",
  "https://github.com/loanBRNT/delivery_packing_python",
  [86, 122, 123, 124],
  "IsaacSim",
  "Multi-Robot",
  "Language",
  [126, 127],
  "hackathon",
  "open-source",
  0,
  "> ðŸ—ï¸ Content on this page is a work in progress and will be updated soon.\n\nThis project was initiated during a robotics hackathon organized by [REVEL](https://revelstudios.io/) and [LycheeAI](https://lycheeai-hub.com/), with the objective of building a useful manipulation-oriented robotic application.\n\nThe proposed scenario focuses on hospital environments, where nursing staff are frequently burdened with non-critical logistical tasks. The goal of the project is to explore how robotic systems could assist by handling simple requests such as delivering food, drinks, or everyday items, allowing nurses to focus on higher-value medical care.\n\nThe system is designed around natural language interaction, enabling patients to express requests in a flexible manner, which are then interpreted and executed by one or more robotic agents within a simulated hospital environment.\n\n## System Overview\n\nTHe project shows two core elements, both of them integrated in Isaac Sim:\n\n- **Robot Collaboration:** I create a program able to manage and synchronize multi-robot via a state machine to let a manipulator robot and a mobile robot to coordinate their work to deliver as fast as possible objects to patient.\n- **Language-control:** Building a simple pipeline to transform patient instruction in natural language in a robotics action. Staying efficient enough to run on laptop pc.",
  "site/content/projects/hospital-4-0.md",
  "a0056c31b567de91",
  { "html": 133, "metadata": 134 },
  "\u003Cblockquote>\n\u003Cp>ðŸ—ï¸ Content on this page is a work in progress and will be updated soon.\u003C/p>\n\u003C/blockquote>\n\u003Cp>This project was initiated during a robotics hackathon organized by \u003Ca href=\"https://revelstudios.io/\" rel=\"noopener noreferrer\" target=\"_blank\">REVEL\u003C/a> and \u003Ca href=\"https://lycheeai-hub.com/\" rel=\"noopener noreferrer\" target=\"_blank\">LycheeAI\u003C/a>, with the objective of building a useful manipulation-oriented robotic application.\u003C/p>\n\u003Cp>The proposed scenario focuses on hospital environments, where nursing staff are frequently burdened with non-critical logistical tasks. The goal of the project is to explore how robotic systems could assist by handling simple requests such as delivering food, drinks, or everyday items, allowing nurses to focus on higher-value medical care.\u003C/p>\n\u003Cp>The system is designed around natural language interaction, enabling patients to express requests in a flexible manner, which are then interpreted and executed by one or more robotic agents within a simulated hospital environment.\u003C/p>\n\u003Ch2 id=\"system-overview\">\u003Ca class=\"heading-link\" aria-label=\"Link to section\" href=\"#system-overview\">#\u003C/a>System Overview\u003C/h2>\n\u003Cp>THe project shows two core elements, both of them integrated in Isaac Sim:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Robot Collaboration:\u003C/strong> I create a program able to manage and synchronize multi-robot via a state machine to let a manipulator robot and a mobile robot to coordinate their work to deliver as fast as possible objects to patient.\u003C/li>\n\u003Cli>\u003Cstrong>Language-control:\u003C/strong> Building a simple pipeline to transform patient instruction in natural language in a robotics action. Staying efficient enough to run on laptop pc.\u003C/li>\n\u003C/ul>",
  {
    "headings": 135,
    "localImagePaths": 137,
    "remoteImagePaths": 138,
    "frontmatter": 139,
    "imagePaths": 142
  },
  [136],
  { "depth": 66, "slug": 100, "text": 101 },
  [],
  [],
  {
    "title": 117,
    "description": 118,
    "video": 119,
    "github": 120,
    "tags": 140,
    "types": 141,
    "order": 128
  },
  [86, 122, 123, 124],
  [126, 127],
  [],
  "about",
  ["Map", 53, 145],
  {
    "id": 53,
    "data": 146,
    "body": 148,
    "filePath": 149,
    "digest": 150,
    "rendered": 151
  },
  { "title": 147 },
  "Because Intelligence starts with curiosity",
  "I am a **PhD Researcher in Robotics** working on foundation models and long-horizon manipulation. Most of my days are spent thinking about language-conditioned tasks and how multi-agent systems can work together. I believe that true '*intelligence*' won't emerge simply from scaling parameters, but through **System-level Intelligence**. Because of this, I want to build robust, modular systems that don't just work in a paper, but actually interact with real people on real hardware.\n\n### Beyond the lab\n\nFor me, research doesnâ€™t end as a paper. Iâ€™ve always been a challenger, constantly pushing myself to take ideas out of my head and into the real world. My curiosity usually starts with a 'why' or a 'it would be so cool to have that' and quickly turns into 'how can I build this?'. Whether Iâ€™m working on a robotic pipeline or a weekend side project, Iâ€™m driven by making things that are useful and grounded in the real world.\n\nI share these experiments and thoughts on **[Curious Intelligence](https://www.youtube.com/@loanbernat8145)**, my YouTube channel. Itâ€™s a place where I can be less formal, explore news ideas, and connect with people who are just as obsessed with the future of robotics as I am.\n\nIâ€™m always open to collaborating on projects that push the boundaries of multi-agent systems and robotic autonomy. Feel free to reach out.",
  "site/content/about/index.md",
  "9138628af079f6ab",
  { "html": 152, "metadata": 153 },
  "\u003Cp>I am a \u003Cstrong>PhD Researcher in Robotics\u003C/strong> working on foundation models and long-horizon manipulation. Most of my days are spent thinking about language-conditioned tasks and how multi-agent systems can work together. I believe that true â€˜\u003Cem>intelligence\u003C/em>â€™ wonâ€™t emerge simply from scaling parameters, but through \u003Cstrong>System-level Intelligence\u003C/strong>. Because of this, I want to build robust, modular systems that donâ€™t just work in a paper, but actually interact with real people on real hardware.\u003C/p>\n\u003Ch3 id=\"beyond-the-lab\">\u003Ca class=\"heading-link\" aria-label=\"Link to section\" href=\"#beyond-the-lab\">#\u003C/a>Beyond the lab\u003C/h3>\n\u003Cp>For me, research doesnâ€™t end as a paper. Iâ€™ve always been a challenger, constantly pushing myself to take ideas out of my head and into the real world. My curiosity usually starts with a â€˜whyâ€™ or a â€˜it would be so cool to have thatâ€™ and quickly turns into â€˜how can I build this?â€™. Whether Iâ€™m working on a robotic pipeline or a weekend side project, Iâ€™m driven by making things that are useful and grounded in the real world.\u003C/p>\n\u003Cp>I share these experiments and thoughts on \u003Cstrong>\u003Ca href=\"https://www.youtube.com/@loanbernat8145\" rel=\"noopener noreferrer\" target=\"_blank\">Curious Intelligence\u003C/a>\u003C/strong>, my YouTube channel. Itâ€™s a place where I can be less formal, explore news ideas, and connect with people who are just as obsessed with the future of robotics as I am.\u003C/p>\n\u003Cp>Iâ€™m always open to collaborating on projects that push the boundaries of multi-agent systems and robotic autonomy. Feel free to reach out.\u003C/p>",
  {
    "headings": 154,
    "localImagePaths": 159,
    "remoteImagePaths": 160,
    "frontmatter": 161,
    "imagePaths": 162
  },
  [155],
  { "depth": 156, "slug": 157, "text": 158 },
  3,
  "beyond-the-lab",
  "#Beyond the lab",
  [],
  [],
  { "title": 147 },
  []
]
